# Ollama Configuration
OLLAMA_URL=http://localhost:11434/api/chat

# Context Management Configuration
# Maximum tokens for context window (default: 32000)
CONTEXT_MAX_TOKENS=32000

# Ratio of tokens to reserve for model replies (default: 0.25 = 25%)
CONTEXT_REPLY_RESERVE_RATIO=0.25

# Number of recent messages to keep during condensation (default: 8)
CONTEXT_RECENT_WINDOW_SIZE=8

# Examples for different LLM contexts:
# For GPT-4 Turbo (128k context):
# CONTEXT_MAX_TOKENS=100000

# For Claude 3 (200k context):
# CONTEXT_MAX_TOKENS=150000

# For Llama 2 (4k context):
# CONTEXT_MAX_TOKENS=3200

# For Code Llama (16k context):
# CONTEXT_MAX_TOKENS=12800